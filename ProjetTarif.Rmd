---
title: "Projet de tarification"
author: "Mikael BOZON, Pierre MARJOLET, William LAURENT, Oskar LAVERNY"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE,}
### Options knitR par d?ffaut pour tout les chunks : 
    knitr::opts_chunk$set(echo = FALSE,warning = FALSE)

### Instalation des packages n?c?ssaires : si vous avez besoin de CASdataset d?commentez la ligne : 
     #install.packages("CASdatasets", repos = "http://dutangc.free.fr/pub/RRepos/", type="source") # ligne sp?ciale pour CASdatasets qui ne vient pas de la meme source (pas de CRAN)
    .list.of.packages <- c("CASdatasets","dplyr", "xts", "sp", "magrittr","ggplot2","gridExtra","gtable","grid","tidyr", "purrr","broom","pscl")
    .new.packages <- .list.of.packages[!(.list.of.packages %in% installed.packages()[,"Package"])]
    if(length(.new.packages)) install.packages(.new.packages)
    lapply(.list.of.packages,function(x){library(x,character.only=TRUE)}) 
    
### Formating color in markdown : doit être utiliser comme ?a : `r colFmt("MY RED TEXT",'red')`
    .colFmt = function(x,color){
      outputFormat = rmarkdown::all_output_formats(knitr::current_input())#opts_knit$get("rmarkdown.pandoc.to")
      if(outputFormat == 'pdf_document')
        paste("\\textcolor{",color,"}{",x,"}",sep="")
      else if(outputFormat == 'html_document')
        paste("<font color='",color,"'>",x,"</font>",sep="")
      else
        x
      print("warnings dont matter, the point was to use the first condition")
    }
```

# TARIFICATION PAR MODELES LINEAIRES GENERALISES EN ASSURANCE NON VIE : Consignes

**Objectif** : proposer un tarif en appliquant diff?rentes m?thodes de tarification (fr?quence-coût moyen ou non) vues en cours impliquant l&#39;usage des modèles lin?aires g?n?ralis?s (GLM). Ce tarif pourra (ou non) être segment? par profil pour tenir compte de l&#39;h?t?rog?n?it? du risque.

**Logiciel utilis?** : logiciel R (logiciel libre Open Source de statistiques).

**Contexte** : ? partir d&#39;une base sinistre d&#39;un portefeuille r?el d&#39;assurance non vie, plus pr?cis?ment en assurance automobile (garantie Responsabilit? Civile). Les bases SINISTRE et CONTRAT ? ?tudier sont contenues dans la librairie **CASdatasets** de R.

Les donn?es ? importer s&#39;intitulent **freMTPL2freq** et **freMTPL2sev**.

La signification des variables est fournie dans les fichiers d&#39;aide de R.

Les caract?ristiques sur les v?hicules et les assur?s ont ?t? collect?es lors de la phase de souscription des contrats.

`r .colFmt("La fa?on de mener le projet (m?thodes envisag?es, test?es, appr?ciation des r?sultats) est volontairement laiss?e très libre afin de responsabiliser les ?tudiants et leur permettre de d?velopper un esprit critique.",'red')`



**Informations pratiques** : le projet se fait par groupe de 4 (aucun travail impliquant un nombre diff?rent d&#39;?tudiants ne sera not?, hormis pour un groupe si le nombre d&#39;?tudiants global n&#39;est pas un multiple de 4). Il est ? rendre pour le 14 janvier 2018 avril minuit maximum en renvoyant ? l&#39;adresse [xavier.milhaud@univ-lyon1.fr](mailto:xavier.milhaud@univ-lyon1.fr) les documents suivants:

- --un rapport qui d?crit votre approche et vos r?sultats,
- --le script R **comment?** qui vous a servi ? obtenir vos r?sultats.

Ce rapport ne devra pas d?passer 6 pages (sans compter la page de garde si vous en mettez une, et hors annexes ?ventuelles où des graphiques ou autres informations peuvent être ins?r?s). La taille des annexes est ?galement limit?e ? 5 pages.

**Les fichiers envoy?s devront être nomm?s de la fa?on suivante: nom1-nom2-nom3-nom4.extension**

**`r .colFmt("Vous devez vous inspirer du travail r?alis? en TP de tarification.",'red')`**

**Etapes du projet** : fusionner les bases &quot;Sinistres&quot; et &quot;Contrats&quot; de manière ? faire le lien entre les montants de sinistre et les contrats qui y sont li?s.



Voici les grandes ?tapes attendues ? titre indicatif, mais vous pouvez bien sûr d?velopper d&#39;autres ?tapes personnelles:

1. 1)Proposer des statistiques descriptives sur l&#39;ensemble des informations de la base de donn?es: tableaux d&#39;effectifs; indicateurs statistiques classiques; indicateurs de corr?lation; densit?s… En d?duire des messages et des actions ? mener pour la mod?lisation ? venir.

1. 2)Estimer des modèles de tarification pour expliquer la charge sinistre en fonction des facteurs de risque. Sur la base d&#39;un raisonnement justifi? et clair, optimiser le modèle de fa?on ? retenir le modèle qui vous parait le meilleur selon un critère que vous expliciterez.

1. 3)Vous ne retiendrez ? la fin qu&#39;une unique mod?lisation parmi l&#39;ensemble des types de mod?lisation test?es. Expliquez et justifiez votre choix.

1. 4)En d?duire un tarif individualis? en fonction des caract?ristiques des assur?s. Interpr?ter les r?sultats de manière concrète, et comparer ces r?sultats ? votre historique.

1. 5)Apportez une vision critique de votre mod?lisation, et donnez des pistes d&#39;am?lioration potentielles si cela est possible.



# TARIFICATION PAR MODELES LINEAIRES GENERALISES EN ASSURANCE NON VIE : Réalisation


## Récupération des données

Les bases SINISTRE et CONTRAT à étudier sont contenues dans la librairie **CASdatasets** de R. Les données à importer s'intitulent **freMTPL2freq** et **freMTPL2sev**.

```{r R?cup?ration des donn?es,output=FALSE}

# On r?cupère les deux datasets : 
  data(freMTPL2freq)
  data(freMTPL2sev)

# on les renomes pour sauvegarder leur version originale : 
  data.freq <- freMTPL2freq
  data.sev <- freMTPL2sev

#finalement on peut les regarder : 
  head(data.freq, n = 20)
  head(data.sev, n = 20)


```


On a donc à notre disposition les variables suivantes : 


* Dans la base sinistre : 
    * IDpol : Identifiant police > ne serivra pas a l'analayse, juste a merger les deux bases.
    * ClaimNb : Nombre de sinistre déclarés sur la periode d'exposition > Reste un nombre, variable a prédire pour la partie fréquance.
    * Exposure : Durée d'exposition, en années > sera un Offset pour la partie fréquance.
    * Area : Inditifiant "Area code" > déja un factor, peut servir a faire un zonnier, mais pas obligatoire.
    * VehPower : Puissance du véhicule, ordonnée en catégories > doit être un factor, variable explicative.
    * VehAge : L'age du véhicule, en années > on peut faire des classes d'age pour récupérer un factor
    * DrivAge : L'age du conducteur (en france, on peut conduire a partir de 18ans) > Idem, des classes d'ages
    * BonusMalus : Coefficient Bonus/malus, entre 50 et 350. > On peut le garder en integer, je ne sais s'il faut el considérer comme une vairbale explicative, ou plutot l'appliquer a la fin sur la prime.
    * VehBrand : La marque du véhicule (catégories inconues) > reste un factor
    * VehGas : le type de carburant, "Diesel" ou "Regular" > devient un factor
    * Density : Nombre d'habitant par km2 dans la ville ou habite le conducteur > aucune idée, peut serivr a plusieurs choses.
    * Region : Les régions de polices, basées sur la classification standard française > clairment un facteur, sert a faire un zonnier.
* Dans la base contrat : 
    * IDpol : Identifiant police
    * ClaimAmout : Cout total du sinistre, vu a une date récente ( à l'ultime).


# Partie fréquence 

```{r "Base fréquence : typage des champs et regrouppement de modalitées",include=FALSE}

################################################
######################## Mise en place de l'environnement
################################################



# ACHTUNG : Je n'ai pas commencer par poser un dataset d'échantillogane et un dataset de validation. Il faudrais le faire. 
# Ainsi tout ce que j'ai fait ici est en fait a faire uniquement sur le dataset d'apprentissage. 
# De plus, j'ai séparer dans deux chunks les regrouppements de modalitées et les fitting de modèles. Pas sur qu'il fallais faire comme ça. J'ai peut d'avoir trop regroupper car mes modèles fittent très bien. Ou alors c'est juste que la BDD est enorme. 


# A propos des graphiques : 
# Plutot que de faire simplement des histogrames, en comptant 1 dans chaque observation, il faurais compter l'exposure de chaque observation... Je ne sais pas comme faire. 
# Ou plutot si, je sais




# Commençons par réduire la taille de la data pour pouvoir travailler tranquilement si besoin. (pas obligatoire)
# df <- sample_frac(data.freq,0.05)
df <- data.freq
# Re-factorisons certaines variables :  
str(df)
cols <- c("VehGas")
df[cols] <- lapply(df[cols], function(x) factor(x))
str(df)

par(mfrow=c(2,2))



# Fonction qui fabrique les plots : prérequis : 
makeplot <- function(data,var,title="Before",continuous=TRUE){
  
  if(continuous){
  data %>% 
    ggplot(aes(x = data[,var],weight = Exposure,fill=as.factor(ClaimNb)), environment = environment()) +
      geom_histogram() +
      ggtitle(title) %>%
    return
  } else{
  data %>% 
    ggplot(aes(x = data[,var],weight = Exposure,fill=as.factor(ClaimNb)), environment = environment()) +
      geom_bar() +
      ggtitle(title) %>%
    return
  }
  
}


################################################
######################## Traitement de l'age du v?hicule
################################################
# Graphique de l'?tat du monde : 
df %>% makeplot("VehAge") -> .VehAgeBefore
  
# Ok donc on peut clairement cr?er une classe 30+
#df[df$VehAge<30,] %>% 
#  qplot(data=.,VehAge,fill = as.factor(ClaimNb)) 
#et des classes plus petites de 5 ann?es chacunes entre 0 et 20, puis 20-30.

df$VehAge %<>% 
  cut(., breaks = c(0, 5, 10, 15, 20, 30, max(.)), include.lowest = TRUE) %T>%
  {print(summary(.))}
# Graphique de l'?tat du monde Apr?s : 
df %>% makeplot("VehAge",title = "After",continuous=FALSE) -> .VehAgeAfter 

################################################
######################## Traitement de l'Age du conducteur
################################################
# Graphique de l'?tat du monde : 
df %>% makeplot("DrivAge") -> .DrivAgeBefore
# Ok ce coup ci c'est moins flagrant. 
# On va faire des classes d'age classqiue 18-25 puis 25-35,35-45, etc. jusqu'a 75+ ou 85+ en fonction du nombre de pelo

#df[df$DrivAge > 75,] %>% 
#  qplot(data=.,DrivAge,fill = as.factor(ClaimNb)) # Ok donc on prend 85+, mais peut-être qu'on les rassemblera plus-tard (pour des problème de manque de donn?es.)

df$DrivAge %<>% 
  cut(., breaks = c(18, seq(from = 25,to = 85,by = 10), max(.)), include.lowest = TRUE) %T>%
  {print(summary(.))}

# Graphique de l'?tat du monde Apr?s : 
df %>% makeplot("DrivAge",title="After",continuous=FALSE) -> .DrivAgeAfter 
################################################
######################## Traitement de la puissance du v?hicule
################################################
df %>% makeplot("VehPower") -> .VehPowerBefore 

# Je propose de les regroupp?es 2 par 2 . 
table(df$VehPower)
df$VehPower %<>% 
  cut(., breaks = c(4,5,6,7,8,11,15), include.lowest = TRUE,right=FALSE) %T>%
  {print(summary(.))}
df %>% makeplot("VehPower",title="After",continuous=FALSE) -> .VehPowerAfter


################################################
######################## Traitement de la marque du véhicule 
################################################
# Commençons par les normaliser : 
table(df$VehBrand)
df$VehBrand %<>% 
  as.character %>% 
  strsplit("B") %>% 
  do.call(rbind,.) %>% 
  {as.numeric(.)[(nrow(df)+1):(2*nrow(df))]}
table(df$VehBrand)

# Puis réduisons le nombre de modalitées : Il semble en effet que les codes soit ordonnés.
df %>% makeplot("VehBrand",continuous=FALSE) -> .VehBrandBefore

# APrès un premier regrouppement 2 par 2 qui ne fittais pas sur la fréquence, on tente unregrouppement en 3 classes : 
table(df$VehBrand)
df$VehBrand %<>% 
  cut(., breaks = c(1,3,7,11), include.lowest = TRUE,right=FALSE) %T>%
  {print(summary(.))}


df %>% makeplot("VehBrand",title="After",continuous=FALSE) -> .VehBrandAfter
# Les variables créers VariableBefore et VariableAfter contienne des graphiques, que l'on plottera plus tard. 
# Les modifications des variables ont été dirrectement appliquées à la base de données 'df', ces variables servent juste a l'affichage.

################################################
######################## Remise en route du dataset avec les regrouppements faits.
################################################

# Regardons la tete du dataset : 
str(df)

# Une fois les regrouppements faits, on peut considérer qu'on va les utiliser ET pour la fréquence ET pour le cout moyen. 
# Remettons donc la bdd obtenue dans ata.freq : 
data.freq <- df


```


Apr?s analyse du dataset, nous avons discr?tiser certains variables quatitatives via les classes de disc?tisation suivantes : 

* Pour l'age du v?hicule : `r names(table(df$VehAge))`
* Pour l'age du conducteur  : `r names(table(df$DrivAge))`
* Pour la puissance du v?hicule : `r names(table(df$VehPower))`
* Pour le carburant : `r names(table(df$VehGas))`

En effet, au vue des graphiques suivant donnant la densit?e de chaque Nombre de sinistre en fonction de ces variables, ces classifications nous ont parues logiques : 

```{r fig.align="center", fig.width=10}
# Affichage des différents regroupements de modalité effectués.
# lors de l'execution finale ou tout simplement pour voir ce qu'il c'est passer, décommenter les lignes suivantes : 

#grid.arrange(.VehAgeBefore, .VehAgeAfter, nrow=2, ncol=1) # clairement, toujours pas.
#grid.arrange(.DrivAgeBefore, .DrivAgeAfter, nrow=2, ncol=1) # clairement, la non plus.
#grid.arrange(.VehPowerBefore, .VehPowerAfter, nrow=2, ncol=1) # clairement, ça va pas non plus.
#grid.arrange(.VehBrandBefore, .VehBrandAfter, nrow=2, ncol=1) # clairement, ça va pas.
```


Maintenant que nos discrétisations sont faites, appliquons un relevel sur data.freq pour créer un profil de référence : On prend pour chaque variable la modalitée la plus représentée :
```{r profilDeReference}
# Le but va être de relevel automatique tout les factor du dataset sur la valeur la plus représentée : 

# petite fonction : 
Autorelevel <- function(dataset){
  
    # On commence par construire le profil de référence.
    .ProfilDeRef <- 
      dataset %>%
      Filter(is.factor,.) %>%
      map(table) %>%
      map(sort,decreasing = TRUE) %>%
      map(names) %>%
      map_chr(1)
    
    # Puis pour chaque facteur, on relever sur le profil de référence.
    for (i in names(Filter(is.factor,dataset))){
      dataset[,i] <- relevel(dataset[,i],.ProfilDeRef[i])
    }
    return(dataset)
}

# on applique a notre dataset : 
data.freq %<>% Autorelevel


```








Une fois ces discrétisations primaire effectuées, nous allons essayer de fitter un modèle GLM log-poisson sur la fréquence. 

Des ramifications ent erme de Zero-inflated, de Over-dispersed quasi-poisson, de Negative binomial ou de tout cela en meme temps seront ensuite possible. 
Un ajout de la version de renormalisation utilisée en TD sera aussi possible. 

Si souhaiter, on pourra aussi mettre a par la variable géographique pour la traiter en terme de zonnier. 

On va donc fitter le nombre de sinistres sur le reste des variables. 


Il faudrais aussi mettre en place un échantillon de validation et un échantillon d'apprentissagE. 
Les choix de ces échantillons peuvent être faits par bootstrap, par exemple : 
On choisis aléatoirement des échantillons, on fitte les modèles sur ces échantillons et on prend en modèle moyen. 

Une sorte de GLM bootstrapé. Why not :)




```{r Renormalisation de la fr?quence}

# ici on continue d'utiliser data.freq, car on travaille surla fréquence.

# Attention, on met l'exposure en offset 
# On utilise ici une liste "mod.freq" qui contiendra tout les modèles qu'on va fabriquer sur la partie frequence.
mod.freq <- list()

# commençons par poser un modèle sur la BDD de fréquence non modifiée, brutalement avec stepAIC : 
# mod.freq$Brutal<- freMTPL2freq %>% 
#   select(-c(IDpol)) %>%
#   glm (data = ., ClaimNb ~ . , offset=log(Exposure), family=poisson(link=log)) %T>%
#   {print(summary(.))}



#names(data.freq)
# IDpol + ClaimNb + Exposure + Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region


################################################
######################## Modèles Poissoniens
################################################
# Passons a la BDD avec les regrouppements de modalité appliqué : Par déffaut on vire IDPOL

mod.freq$poissonLog<- data.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

mod.freq$poissonLog2<- data.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}


#AreaF, AreaB et Density ne fittent pas bien. 
# retirons density : 
mod.freq$poissonLog3<- data.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas, offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}
#Ok c'est un tout petit peu mieux. Density explique probablement AreaF, ou en tout cas elles apporte la meme information.

# mais tout le reste est plutot bon. 

################################################
######################## Modèles poissoniens surdispersés
################################################
mod.freq$odPoissonLog<- data.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas , offset=log(Exposure), family=quasipoisson(link=log)) %T>%
  {print(summary(.))}

# le paramètre de dispertion est très proche de 1 !!! 
# C'est OUF, c'est très rare d'avoir des données qui ne sont pas overdispersé.

# on a es problèmes avecles Area, car les AreaB et AreaF sont moins représentées que les autres. 
# il faudrais peut-être regroupper.
#Bon OK meme en regrouppant ça fitte pas. Donc on les dégage.
mod.freq$odPoissonLog2<- data.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas , offset=log(Exposure), family=quasipoisson(link=log)) %T>%
  {print(summary(.))}

mod.freq$PoissonLog4<- data.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}


################################################
######################## Modèles ZIP
################################################

# mod.freq$ZeroInflPoissonLog<- data.freq %>% 
#   select(-c(IDpol)) %>%
#   zeroinfl(data = ., formula = ClaimNb ~ . + offset=log(Exposure), dist="poisson") %T>%
#   {print(summary(.))}
# 


#### Echec : depuis le début je traite l'exposure en offset, mais je la met dans les GLM !!! Ou peut-être pas ? 
#Testons cela : 

data = data.frame(Y = c(1,2,3,4),
               X1 = c(3,4,5,6),
               X2 = c(FALSE,FALSE,TRUE,TRUE),
               Expo = c(0.1,0.2,0.3,0.5))

glm(data=data,formula=Y ~ .,offset=log(Expo),family=poisson(link=log))







# Tout mes modèles ont pour l'instant un AIC supperieur au poisson stanard. 

# enfait on pourrais meme en rajouter pleins d'autres. A la fin on fera un classement selon un critère objectif pour retenir le meilleur. 
# on pourra essayer selon un AIC, un BIC ou autre :)

# Ok je vais faire ça. 
# Je pense que j'ai trop regroupper a priorit. Je vais essayer en supprimant un regroupement ou deux, et juste en mettant en facteur certaines variables. ou alors en faisant des regroupement moins violents, par exemple un pas e discretisation de 5 au lieu e 10 pourles ages ou qqch comme ça. 





# Comparons les résultats : 
dfMod.freq <- 
  tibble(Nom = names(mod.freq)) %>%
  mutate(
    Mod = mod.freq,
    family = map(Mod,"family"),
    glance = Mod %>% map(glance),
    tidy = Mod %>% map(tidy)
  )

# On peut par exemple prendre le modèle moyen, dans chaque famille.
dfMod.freq %>% unnest(glance)
dfMod.freq %>% unnest(tidy)



# je vient de faire un truc de ouf : j'ai mit des GLM dans un data.frame, et avec la meme methode ( si on oublis les unnest) je met des df dans des df :)



```




# Partie Couts moyens 
 
 
```{r "Base sévérité : typage des champs et regrouppement de modalit?es",include=FALSE}

#_________La première étape est de jointer les bases de données sur la IDPol

# c'est partit : 
data <- full_join(data.freq,data.sev)

#On ne retient que les polices sinistrées
sev <- subset(data, ClaimNb > 0)


#On exclue les ligne sans montant de sinistre: il y en a 9000 environ
sev <- subset(sev, ClaimAmount > 0)


#________Voici venu le temps de s'intéresser à la mise en forme et à l'analyse des données de cette base

# plutot que de te retaper les factorisaisons des vairbales, je les ai remise plus haut dans data.freq, comme ça tu n'a plus a les gérer ici. 









```



# Partie Remise ensemble, pr?diction, etc..








Maintenant que nos variables explicatives sont bien leveled, on va rassembler les deux bases.

Pas sur que ce soit tr?s interessant e merger les deux bases. 
On va plutot fitter s?par?ment les deux . 

```{r Merging des donn?es}

# Verifions tout d'abord que les IDpol dispo dans la base de contrat (celle donnant les montant le sinistres, data.sev) sont bien unique dans la première base : 

# Il y a surrement  une correpondance 1/inf entre les identifiant police des deux bases 'IDpol'. EN d'autres termes, la base sinistre contient une ligne par sinistre et les montants pr?sent dedans sont donc des montant individuel de sinistre et non pas agr?g?. Arguments sur le choix du type de jonction : 
      # dim(left_join(data.freq,data.sev))[1] == dim(data.freq)[1]  # doit retourner faux.
      # prod(unique(data.freq$IDpol) == data.freq$IDpol) == 1 # doit retourner vrai
      # prod(unique(data.sev$IDpol) == data.sev$IDpol) == 1 # doit retourner faux
      # 
      # sum(data.freq$ClaimNb) == nrow(data.sev) # ?a m'aurais arranger que ?a coresponde !!! Malheureusement on est pas dans une relation 1/Inf parfaite...
      # 
      # # essayons autrement : 
      # data.freq %>% group_by(ClaimNb) %>% summarise(n=n())
      # data.sev %>% group_by(IDpol) %>% summarize(count = n()) %>% group_by(count) %>% summarise(n=n())
      # # Con de sa race les deux bases ne corespondent pas. 
      # # chequons les lignes qui ne sont pas complete et qui ont un claimNb diff?rent de 0 
      # test.join <- left_join(data.freq,data.sev) %>% filter(!complete.cases(.),ClaimNb != 0)
      # #Ok on a donc a peu près 9000 ligne e sinistre sans valeur ou de sinistres sans donn?es. 
      # test.join %>% filter(is.na(ClaimAmount))
      # # Ha ce ne sont que des sinistres sans valeurs. 
      # 
      # # Ok c'est pas si grave, on les excluera juste de l'analyse.
      # #ernier petit test : 
      # nrow(left_join(data.freq,data.sev)) == nrow(full_join(data.freq,data.sev))
      # # Bon finalement le left_join n'est pas indiqu?. On va plutot prendre un full_join.

# c'est partit : 
data <- full_join(data.freq,data.sev)




```{r EstimationFrequence}















```



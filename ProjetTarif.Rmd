---
title: "Projet de tarification"
author: "Mikael BOZON, Pierre MARJOLET, William LAURENT, Oskar LAVERNY"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE,}
### Options knitR par déffaut pour tout les chunks : 
    knitr::opts_chunk$set(echo = FALSE,warning = FALSE)

### Instalation des packages nécéssaires : si vous avez besoin de CASdataset décommentez la ligne : 
    # install.packages("CASdatasets", repos = "http://dutangc.free.fr/pub/RRepos/", type="source") # ligne sp?ciale pour CASdatasets qui ne vient pas de la meme source (pas de CRAN)
    .list.of.packages <- c("CASdatasets","dplyr", "xts", "sp", "magrittr","ggplot2","gridExtra","gtable","grid","tidyr", "purrr","broom","pscl","forcats","corrplot","fExtremes")
    .new.packages <- .list.of.packages[!(.list.of.packages %in% installed.packages()[,"Package"])]
    if(length(.new.packages)) install.packages(.new.packages)
    lapply(.list.of.packages,function(x){library(x,character.only=TRUE)}) 
    
### Formating color in markdown : doit être utiliser comme ça : `r colFmt("MY RED TEXT",'red')`
    .colFmt = function(x,color){
      outputFormat = rmarkdown::all_output_formats(knitr::current_input())#opts_knit$get("rmarkdown.pandoc.to")
      if(outputFormat == 'pdf_document')
        paste("\\textcolor{",color,"}{",x,"}",sep="")
      else if(outputFormat == 'html_document')
        paste("<font color='",color,"'>",x,"</font>",sep="")
      else
        x
      print("warnings dont matter, the point was to use the first condition")
    }
    
```

# TARIFICATION PAR MODELES LINEAIRES GENERALISES EN ASSURANCE NON VIE : Consignes

**Objectif** : proposer un tarif en appliquant diff?rentes m?thodes de tarification (fréquence-coût moyen ou non) vues en cours impliquant l'usage des modèles linéaires généralisés (GLM). Ce tarif pourra (ou non) être ségment? par profil pour tenir compte de l'apétence du risque.

**Logiciel utilis?** : logiciel R (logiciel libre Open Source de statistiques).

**Contexte** : ? partir d'une base sinistre d'&#39;'un portefeuille r?el d&#39;assurance non vie, plus précisément en assurance automobile (garantie Responsabilité Civile). Les bases SINISTRE et CONTRAT ? étudier sont contenues dans la librairie **CASdatasets** de R.

Les données ? elles s'intitulent **freMTPL2freq** et **freMTPL2sev**.

La signification des variables est fournie dans les fichiers d'aide de R.

Les caractéristiques sur les véhicules et les assurés ont été collect?es lors de la phase de souscription des contrats.

`r .colFmt("La façon de mener le projet (méthodes envisagées, testées, appréciation des résultats) est volontairement laissée très libre afin de responsabiliser les étudiants et leur permettre de développer un esprit critique.",'red')`



**Informations pratiques** : le projet se fait par groupe de 4 (aucun travail impliquant un nombre différent d'&#39;'étudiants ne sera noté, hormis pour un groupe si le nombre d'étudiants global n'est pas un multiple de 4). Il est à rendre pour le 14 janvier 2018 avril minuit maximum en renvoyant à l'adresse [xavier.milhaud@univ-lyon1.fr](mailto:xavier.milhaud@univ-lyon1.fr) les documents suivants:

- --un rapport qui décrit votre approche et vos résultats,
- --le script R **commenté** qui vous a servi à obtenir vos résultats.

Ce rapport ne devra pas dépasser 6 pages (sans compter la page de garde si vous en mettez une, et hors annexes éventuelles où des graphiques ou autres informations peuvent être insérés). La taille des annexes est également limitée à 5 pages.

**Les fichiers envoyés devront être nommés de la façon suivante: nom1-nom2-nom3-nom4.extension**

**`r .colFmt("Vous devez vous inspirer du travail réalisé en TP de tarification.",'red')`**

**Etapes du projet** : fusionner les bases &quot;Sinistres&quot; et &quot;Contrats&quot; de manière à faire le lien entre les montants de sinistre et les contrats qui y sont liés.



Voici les grandes étapes attendues à titre indicatif, mais vous pouvez bien sûr développer d'autres étapes personnelles:

1. 1)Proposer des statistiques descriptives sur l'ensemble des informations de la base de données: tableaux d'effectifs; indicateurs statistiques classiques; indicateurs de corrélation; densités… En déduire des messages et des actions à mener pour la modélisation à venir.

1. 2)Estimer des modèles de tarification pour expliquer la charge sinistre en fonction des facteurs de risque. Sur la base d'un raisonnement justifié et clair, optimiser le modèle de façon à retenir le modèle qui vous parait le meilleur selon un critère que vous expliciterez.

1. 3)Vous ne retiendrez à la fin qu'une unique modélisation parmi l'ensemble des types de modélisation testées. Expliquez et justifiez votre choix.

1. 4)En déduire un tarif individualisé en fonction des caractéristiques des assurés. Interpréter les résultats de manière concrète, et comparer ces résultats à votre historique.

1. 5)Apportez une vision critique de votre modélisation, et donnez des pistes d'amélioration potentielles si cela est possible.



# TARIFICATION PAR MODELES LINEAIRES GENERALISES EN ASSURANCE NON VIE : Réalisation


## Préliminaires


### Récupération des données.

Les bases SINISTRE et CONTRAT à étudier sont contenues dans la librairie **CASdatasets** de R. Les données à importer s'intitulent **freMTPL2freq** et **freMTPL2sev**.

```{r Récupération des données,output=FALSE}

# On récupère les deux datasets : 
  data(freMTPL2freq)
  data(freMTPL2sev)

```


On a donc à notre disposition les variables suivantes : 


* Dans la base sinistre : 
    * IDpol : Identifiant police > ne serivra pas a l'analayse, juste a merger les deux bases.
    * ClaimNb : Nombre de sinistre déclarés sur la periode d'exposition > Reste un nombre, variable a prédire pour la partie fréquance.
    * Exposure : Durée d'exposition, en années > sera un Offset pour la partie fréquance.
    * Area : Inditifiant "Area code" > déja un factor, peut servir a faire un zonnier, mais pas obligatoire.
    * VehPower : Puissance du véhicule, ordonnée en catégories > doit être un factor, variable explicative.
    * VehAge : L'age du véhicule, en années > on peut faire des classes d'age pour récupérer un factor
    * DrivAge : L'age du conducteur (en france, on peut conduire a partir de 18ans) > Idem, des classes d'ages
    * BonusMalus : Coefficient Bonus/malus, entre 50 et 350. > On peut le garder en integer, je ne sais s'il faut el considérer comme une vairbale explicative, ou plutot l'appliquer a la fin sur la prime.
    * VehBrand : La marque du véhicule (catégories inconues) > reste un factor
    * VehGas : le type de carburant, "Diesel" ou "Regular" > devient un factor
    * Density : Nombre d'habitant par km2 dans la ville ou habite le conducteur > aucune idée, peut serivr a plusieurs choses.
    * Region : Les régions de polices, basées sur la classification standard française > clairment un facteur, sert a faire un zonnier.
* Dans la base contrat : 
    * IDpol : Identifiant police
    * ClaimAmout : Cout total du sinistre, vu a une date récente ( à l'ultime).
    
    

    
    
    
### Jonction des deux bases, gestion des graves

#### Separation des attritionels et des graves

Lors de la jonction des deux bases, nous en avons profitter pour gérer la s'apration des attritionels et des graves. Ci dessout un Mean Excess Plot permettant de fixer le niveau des graves, graçe auquel nous avons fixer le niveau à 200000, nous donnant 0.07% des sinistres comme etant graves.
```{r "Jonction des 2 bases"}

################################################
######################## Récupération des données
################################################
  data <- list()
  data$freq <- freMTPL2freq
  data$sev <- freMTPL2sev
  
  
################################################
######################## Choix du soeuil de grave, séparation Grave / attritionels.
################################################
  
#on identifie directement les sinstres attritionnels des sinistres graves.
  mePlot(data$sev$ClaimAmount)
#Sur le mean excess plot, on d?tecte une rupture pour environ un montant de 200 000??? de sinistre
#Nous retenons cette valeur comme seuil de s?paration des sinistres attritionnels des graves.
  .seuilGrv <- 200000

#les sinistres graves repr?sentent 0.07% de la base de sinistres
  nrow(data$sev[which(data$sev$ClaimAmount>.seuilGrv),])/nrow(data$sev)*100

  
#petit passage interm?diaire pour ajouter une colonne à la base qui permettra de compter le nombre de sinistres au moment de la jointure des bases de donn?es
  data$sev.att <- data.frame(data$sev[which(data$sev$ClaimAmount<=.seuilGrv),],rep(1,nrow(data$sev[which(data$sev$ClaimAmount<=.seuilGrv),])))
  names(data$sev.att) <- c("IDpol","ClaimAmount","AttClaimNb")


  data$sev.grv <- data.frame(data$sev[which(data$sev$ClaimAmount>.seuilGrv),],rep(1,nrow(data$sev[which(data$sev$ClaimAmount>.seuilGrv),])))
  names(data$sev.grv) <- c("IDpol","GrvClaimAmount","GrvClaimNb")


#on agr?ge les sinistres graves, uniquement dans le but d'enrichir la base de donnees generale, ce ne sera pas utilise par la suite
  .sin.grv.nbcum <- aggregate(cbind(GrvClaimNb,GrvClaimAmount) ~ IDpol, data = data$sev.grv, sum)


#Suivant la modélisation retenue (glm brutal ou décomposition fréquence-coût moyen), nous aurons besoins d'utiliser les sinistres cumulés ou la moyenne des sinistres pour chaque police
  .sin.cum <- aggregate(cbind(AttClaimNb,ClaimAmount) ~ IDpol, data = data$sev.att, sum)
  names(.sin.cum) <- c("IDpol","AttClaimNb", "CumAttClaimAmount")
  .sin.mean <- aggregate(ClaimAmount ~ IDpol, data = data$sev.att, mean)
  names(.sin.mean) <- c("IDpol", "MeanAttClaimAmount")
  

#Il ne sous reste plus qu'à joindre la base de frequence avec les 3 autres bases de données
  data$full <- merge(merge(merge(data$freq, 
                                 .sin.grv.nbcum, 
                                 by="IDpol", 
                                 all.x=TRUE),
                           .sin.cum, 
                           by="IDpol", 
                           all.x=TRUE), 
                     .sin.mean, 
                     by="IDpol", 
                     all.x=TRUE)
  


#Les polices non sinistrées présentent logiquement la valeur NA comme montant moyen et cumulé de sinistres.
#Il nous faut les remplacer par 0.
  .clean <- . %>% replace(., is.na(.),0)
  data$full$CumAttClaimAmount   %<>% .clean
  data$full$MeanAttClaimAmount  %<>% .clean
  data$full$AttClaimNb        %<>% .clean
  data$full$GrvClaimNb        %<>% .clean
  data$full$GrvClaimAmount    %<>% .clean

#Pour autant, des polices sinistrées demeurent avec des montants moyens et cumulé de sinistres attritionnel et un nombre de sinistres graves  nuls.
#Nous estimons que ce sont des erreurs dans la base de données. Dans les 2 cas, il nous faut retirer ces lignes de la base.
#Cette ligne de code permet egalement de nettoyer la base des polices dont le nombre de sinistres total est diff?rent de la somme du nombr de sinistres attritionnels et du nombre de sinistres graves
  data$full <- data$full[-which(data$full$ClaimNb !=0 &data$full$ClaimNb != data$full$GrvClaimNb + data$full$AttClaimNb & data$full$CumAttClaimAmount==0),]
  

#Pour que la base de donn?es soit plus simple ? utiliser, nous ne conservons pas le nombre de sinistres total,  mais simplement le nombre de sinstres attritonnel
  data$full$ClaimNb <- data$full$AttClaimNb
  data$full$AttClaimNb <- NULL

################################################
######################## Typage des données
################################################

  str(data$full)
  data$full$ClaimNb <- as.numeric(data$full$ClaimNb) #le nombre de sinistres doit naturellement être de type numérique
  data$full$IDpol <- as.factor(data$full$IDpol) #l'identifiant de la police doit être une variable catégorielle
  data$full$VehPower <- as.numeric(data$full$VehPower) #de même pour la puissance du véhicule
  data$full$VehAge <- as.numeric(data$full$VehAge) #l'âge est toujour numérique
  data$full$DrivAge <- as.numeric(data$full$DrivAge) #idem
  data$full$BonusMalus <- as.numeric(data$full$BonusMalus) #le Bonus Malus est un coefficient qui s'applique à la prime, donc est numérique
  data$full$VehGas <- as.factor(data$full$VehGas) #le carburant de du véhicule est une variable catégorielle
  data$full$Density <- as.numeric(data$full$Density) #la densité est de type numérique
  str(data$full)

```


De plus, certaines nouvelles varibales ont été crées : 
* La variables ClaimNb contient le nombre d'attrionels uniquement
* CumAttClaimAmout : 
* MeanAttClaimAmout : 
* GrvClaimNb : 
* GrvClaimAmout :
    


#### Surprime correspondante aux sinistres graves





```{r "Sinistres graves"}
#On peut d'abord determiner la surprime associee a ces sinitres graves qui sera ajoutee aux resultats du modele developpe pour les sinistres attritionnels
#Cette surprime correspond a la moyenne des sinistres graves sur l'ensemble des assur?s
# on la rajoutera donc a chacun des assurés, (proportionellement a son exposition)

  surprime.sinGrv <- sum(data$sev.grv$GrvClaimAmount)/sum(data$full$Exposure) %T>% print

```

Comme nous sommes dans un modèle de frequence * cout, nous avons decider de gérer les graves a part et de fixer une sur-prime (par unité d'exposition) correspondant a ces sinistres comme la somme des valeurs des graves divisé par l'exposition totale. Cela nous donne une surprime de `r surprime.sinGrv` €.



### Analyse des données disponibles dans la base 



Nous avons commencer par analyser les corrélations entre les différentes variables quantitatives, et nous nous en somme sortit par un test de correlation entre variables qualis. 

[[Commentaire à completer]]

```{r "Analyse des données"}

################################################
######################## Correction des données aberrantes
################################################

#Les données sont annuelles, il est donc anormal de rencontrer des expositions > 1
#Nous supprimons donc toutes les polices présentant une exposition > 1

data$full <- data$full[-which(data$full$Exposure > 1),]

################################################
######################## Etude de la corrélation des données
################################################

###____ Entre variables quantitatives -> matrice de corrélation

str(data$full)
cor_matrix_test <- cor(data$full[,c(2,3,5,6,7,8,11,13,14)], method = "pearson")

diag(cor_matrix_test) <- 0

corrplot(cor_matrix_test)

#Il est intéressant de remarquer que le BonusMalus est negativement corrélé à l'âge du conducteur. Surrement un effet Bonus a vie, ou plus simplement le fait que les jeunes conduisent mal et démarrent avec un bonnus a 100, qui a ensuite tendence a baisser.
#Les variables Cum_ClaimAmount et Mean_ClaimAmount sont logiquement très corrélées

###____ Entre variables qualitatives -> test du khi2

.donnees_quali <- data$full[, c(4,9,10,12)]
str(.donnees_quali)

.nb <- ncol(.donnees_quali)-1
chi2_test_ind <- matrix(0, nrow = .nb, ncol = .nb)
for (.i in 1:.nb){
  for (.j in (.i+1):(.nb+1)){
    chi2_test_ind[.i,.j-1] <- chisq.test(table(.donnees_quali[,.i],.donnees_quali[,.j]))$p.value
  }
}
colnames(chi2_test_ind) <- colnames(.donnees_quali[,-1])
rownames(chi2_test_ind) <- colnames(.donnees_quali[,-4])
abs(chi2_test_ind)>0.05
#elles sont toutes indépendantes, ce qui est bien.


###___ Analyse en composante principale

#A faire


```



### Section Apprentissage / test de la base 


```{r "Section Apprentissage / test de la base"}

################################################
######################## Crétation d'une base de test et d'une base de train
################################################

# paramètre : 
  set.seed(seed=100)
  .Proportion.Train.Wanted = 0.80 # pour des question de rapiditée d'exection, j'ai déscendu la proportion a 0.10, il faut la remonter a 0.8 avent de rendre le code.

# application : 
# 
#   #data$freq$ClaimNb n'est pas un vecteur de la même taille que le jeu de données. réctifions
#   data$full$ClaimNb <- as.vector(data$full$ClaimNb)

  #Je fais une liste d'éléments pris au hazard dans les indices de notre BDD de fréquence
  .index_entrainement <- (1:nrow(data$full)) %>% sample(.,size = .Proportion.Train.Wanted * nrow(data$full))
  
  data$train.full <- data$full[.index_entrainement,]
  data$test.full <- data$full[! seq(from = 1, to = nrow(data$full)) %in% .index_entrainement, ]
  
# retour : 
  Proportion.Train.Achieved <- round(100* nrow(data$train.full) / nrow(data$freq), 2) %T>% print
  
################################################
######################## Crétation d'une fonction d'autorelevel 
################################################

```



Préliminairement aux fitting de modèles, il est important de séparer la base en deux : base d'entrainement et base de data$test.

Tout d'abord nous jointons les deux bases par un full_join, puis nous séparons en base de train et base de data$test. Ensuite, nous créons les bases de fréquence et cout moeyn pour fitter nos eux modèles indépendants : d'un coté les fréquence, pour laquelle nous droppons la colonne cout puis nous droppons toutes les lignes avec des na et les lignes en double, et de l'autre coté les couts pour laquelle nous droppons tout les couts avec des NA et la colonnne Nombre de sinistres, ainsi queles lignes en double.



La base etant assez volumineuse, nous choississons de conserver une proportion de `r .Proportion.Train.Achieved` pour entrainer nos modèles. Ainsi, l'autre patie de la base servira a tester nos modèles avent, une fois valider le framework, de fitter nos modèles sur l'intégralité de la base. 




## Partie fréquence 

### Typage des champs et regrouppements préliminaires

Une des premières choses a faire fut de discrétiser certaines variables continues. 

```{r "Récupération des données et typage des chaps, regrouppements."}
# On récupère la base globale pour la fréquence, avec une partie apprentissage et une partie test.
data$train.freq <- data$train.full
data$test.freq <- data$test.full








#######_____________     Cette partie du code est à reprendre, il faut faire suivre les regroupements de modalité sur la base de test



################################################
######################## Preliminaires aux regrouppements de modalités
################################################

# Chaque regrouppement de modalité effectué sur la base de fréquence devras être fait AUSSI sur la base de test. 
# Ainsi je créer une liste de fonction RegroupFunc.freq$ qui contiendra DANS LORDRE les regrouppeemnts effectués.
# De mème, on peut faire parreil sur la base de cout. 

RegroupFunc.freq <- list()






# Fonction qui fabrique les plots expliquant les regrouppements de modalités choisis : prérequis : 
makeplot <- function(data,var,title="Before",continuous=TRUE){
  
  p <- data %>% ggplot(aes(x = data[,var],weight = Exposure,fill=as.factor(ClaimNb)), environment = environment())
  
  if(continuous){p = p + geom_histogram()}
  else          {p = p + geom_bar()}
  
  p = p + ggtitle(title) + labs(fill="Nombre de Sinitres",x=var,y="Exposition Totale")  
  return(p)
}


################################################
######################## Regrouppement de l'age du v?hicule
################################################
# Graphique de l'?tat du monde : 
data$train.freq %>% makeplot("VehAge") -> .VehAgeBefore
  
# Ok donc on peut clairement cr?er une classe 30+
#df[df$VehAge<30,] %>% 
#  qplot(data=.,VehAge,fill = as.factor(ClaimNb)) 
#et des classes plus petites de 5 ann?es chacunes entre 0 et 20, puis 20-30.




data$train.freq$VehAge %<>% 
  cut(., breaks = c(0, 5, 10, 15, 20, 30, max(.)), include.lowest = TRUE) %T>%
  {print(summary(.))}

# Graphique de l'?tat du monde Apr?s : 
data$train.freq %>% makeplot("VehAge",title = "After",continuous=FALSE) -> .VehAgeAfter 

################################################
######################## Regrouppement de l'Age du conducteur
################################################
# Graphique de l'?tat du monde : 
data$train.freq %>% makeplot("DrivAge") -> .DrivAgeBefore
# Ok ce coup ci c'est moins flagrant. 
# On va faire des classes d'age classqiue 18-25 puis 25-35,35-45, etc. jusqu'a 75+ ou 85+ en fonction du nombre de pelo

#df[df$DrivAge > 75,] %>% 
#  qplot(data=.,DrivAge,fill = as.factor(ClaimNb)) # Ok donc on prend 85+, mais peut-être qu'on les rassemblera plus-tard (pour des problème de manque de donn?es.)

data$train.freq$DrivAge %<>% 
  cut(., breaks = c(18, seq(from = 25,to = 85,by = 10), max(.)), include.lowest = TRUE) %T>%
  {print(summary(.))}

# Graphique de l'?tat du monde Apr?s : 
data$train.freq %>% makeplot("DrivAge",title="After",continuous=FALSE) -> .DrivAgeAfter 
################################################
######################## Regrouppement de la puissance du v?hicule
################################################
data$train.freq %>% makeplot("VehPower") -> .VehPowerBefore 

data$train.freq$VehPower %<>% 
  cut(., breaks = c(4,5,6,7,8,11,15), include.lowest = TRUE,right=FALSE) %T>%
  {print(summary(.))}

data$train.freq %>% makeplot("VehPower",title="After",continuous=FALSE) -> .VehPowerAfter

################################################
######################## Regrouppement d'Area
################################################
data$train.freq %>% makeplot("Area",continuous=FALSE) -> .AreaBefore 

data$train.freq$Area %<>% 
  fct_collapse("AB" = c("A","B"),
               "EF" = c("E","F")) %T>%
  {print(summary(.))}

data$train.freq %>% makeplot("Area",title="After",continuous=FALSE) -> .AreaAfter



################################################
######################## Regrouppement de la marque du véhicule 
################################################
# Commençons par les normaliser : 

data$train.freq$VehBrand %<>% 
  as.character %>% 
  gsub("B", "", .) %>%
  {as.numeric(.)}

table(data$train.freq$VehBrand)

# Puis réduisons le nombre de modalitées : Il semble en effet que les codes soit ordonnés.
data$train.freq %>% makeplot("VehBrand",continuous=FALSE) -> .VehBrandBefore

# APrès un premier regrouppement 2 par 2 qui ne fittais pas sur la fréquence, on tente unregrouppement en 3 classes : 
table(data$train.freq$VehBrand)
data$train.freq$VehBrand %<>% 
  cut(., breaks = c(1,3,7,11), include.lowest = TRUE,right=FALSE) %T>%
  {print(summary(.))}


data$train.freq %>% makeplot("VehBrand",title="After",continuous=FALSE) -> .VehBrandAfter
# Les variables créers VariableBefore et VariableAfter contienne des graphiques, que l'on plottera plus tard. 
# Les modifications des variables ont été dirrectement appliquées à la base de données 'df', ces variables servent juste a l'affichage.
  
################################################
######################## Création du profil de référence
################################################
# Le but va être de relevel automatique tout les factor du dataset sur la valeur la plus représentée : 

# petite fonction : 
Autorelevel <- function(dataset){
  
    # On commence par construire le profil de référence.
    .ProfilDeRef <- 
      dataset %>%
      Filter(is.factor,.) %>%
      map(table) %>%
      map(sort,decreasing = TRUE) %>%
      map(names) %>%
      map_chr(1)
    
    # Puis pour chaque facteur, on relever sur le profil de référence.
    for (i in names(Filter(is.factor,dataset))){
      dataset[,i] <- relevel(dataset[,i],.ProfilDeRef[i])
    }
    
    return(dataset)
}

data$train.freq %<>% Autorelevel


#Reprendre le setup de l'environnement

```




Aprés analyse du dataset, nous avons discrétiser certains variables quatitatives via les classes de disccrétisation suivantes : 

* Pour l'age du véhicule : `r names(table(data$train.full$VehAge))`
* Pour l'age du conducteur  : `r names(table(data$train.full$DrivAge))`
* Pour la puissance du véhicule : `r names(table(data$train.full$VehPower))`
* Pour le carburant : `r names(table(data$train.full$VehGas))`

En effet, au vue des graphiques suivant donnant la densit?e de chaque Nombre de sinistre en fonction de ces variables, ces classifications nous ont parues logiques : 

```{r fig.align="center", fig.width=10}
# Affichage des différents regroupements de modalité effectués.
# lors de l'execution finale ou tout simplement pour voir ce qu'il c'est passer, décommenter les lignes suivantes : 

# grid.arrange(.VehAgeBefore, .VehAgeAfter, nrow=2, ncol=1) # clairement, toujours pas.
# grid.arrange(.DrivAgeBefore, .DrivAgeAfter, nrow=2, ncol=1) # clairement, la non plus.
# grid.arrange(.VehPowerBefore, .VehPowerAfter, nrow=2, ncol=1) # clairement, ça va pas non plus.
# grid.arrange(.VehBrandBefore, .VehBrandAfter, nrow=2, ncol=1) # clairement, ça va pas.
# grid.arrange(.AreaBefore, .AreaAfter, nrow=2, ncol=1) # clairement, ça va pas.
```

### Description de la technique de selection de modèle


#### La technique

#### Les différents modèles fittés

#### Résultat.

Maintenant que nos discrétisations sont faites, appliquons un relevel sur data$freq pour créer un profil de référence : On prend pour chaque variable la modalitée la plus représentée :


Une fois ces discrétisations primaire effectuées, nous allons essayer de fitter un modèle GLM log-poisson sur la fréquence. 

Des ramifications ent erme de Zero-inflated, de Over-dispersed quasi-poisson, de Negative binomial ou de tout cela en meme temps seront ensuite possible. 
Un ajout de la version de renormalisation utilisée en TD sera aussi possible. 

Si souhaiter, on pourra aussi mettre a par la variable géographique pour la traiter en terme de zonnier. 

On va donc fitter le nombre de sinistres sur le reste des variables. 


Il faudrais aussi mettre en place un échantillon de validation et un échantillon d'apprentissagE. 
Les choix de ces échantillons peuvent être faits par bootstrap, par exemple : 
On choisis aléatoirement des échantillons, on fitte les modèles sur ces échantillons et on prend en modèle moyen. 

Une sorte de GLM bootstrapé. Why not :)

```{r "Fitting du modèle de fréquence des attritionels",include=FALSE}




################################################
######################## Setup de l'environnement
################################################

# # Récupération de la bonne base de donnée  de fréquence 
# data$train.freq <- 
#   Databases %>% 
#   filter(Modele=="freq",Type=="train") %>%
#   .$data %>%
#   `[[`(1)

# On utilise ici une liste "mod.freq" qui contiendra tout les modèles qu'on va fabriquer sur la partie frequence.
mod.freq <- list()

################################################
######################## Modèles Poissoniens
################################################

# Attention, on met l'exposure en offset 

#names(data$train.freq)
# IDpol + ClaimNb + Exposure + Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region

# Passons a la BDD avec les regrouppements de modalité appliqué : Par déffaut on vire IDPOL

mod.freq$poissonLog<- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# on vire dirrectement la région : 
mod.freq$poissonLog2<- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# ou alors plutot la puissance du véhicule : 
mod.freq$poissonLog2b<- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region  , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# ha c'est mieux. 
# Et si on vire les deux : 
mod.freq$poissonLog2c<- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density, offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}
# c'est encore mieux.
  
# retirons density : 
mod.freq$poissonLog3 <- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehBrand + VehGas, offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# retirons la marque du véhicule : 
mod.freq$poissonLog4 <- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehGas, offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}
#Ok c'est un tout petit peu mieux. Density explique probablement AreaF, ou en tout cas elles apporte la meme information.

# mais tout le reste est plutot bon. 

################################################
######################## Modèles poissoniens surdispersés
################################################
mod.freq$odPoissonLog<- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas , offset=log(Exposure), family=quasipoisson(link=log)) %T>%
  {print(summary(.))}

# le paramètre de dispertion est très proche de 1 !!! 
# C'est OUF, c'est très rare d'avoir des données qui ne sont pas overdispersé.

# on a es problèmes avecles Area, car les AreaB et AreaF sont moins représentées que les autres. 
# il faudrais peut-être regroupper.
#Bon OK meme en regrouppant ça fitte pas. Donc on les dégage.
mod.freq$odPoissonLog2<- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas , offset=log(Exposure), family=quasipoisson(link=log)) %T>%
  {print(summary(.))}

mod.freq$PoissonLog4<- data$train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}


################################################
######################## Modèles ZIP
################################################

# mod.freq$ZIPLog <-
#   zeroinfl(data = data$train.freq, formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region, offset=log(Exposure), dist="poisson") %T>%
#   {print(summary(.))}

################################################
######################## Modèles ZIBN
################################################
# # Modèle complet : ( attention trèèèèès long a fitter)
# mod.freq$ZIBNLog <-
#   zeroinfl(data = data$train.freq, 
#            formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region, offset=log(Exposure), dist="negbin") %T>%
#            {print(summary(.))}
# 
# 
# # On vire Area dans le modèle de zero-inflation et Region des deux cotés.
# mod.freq$ZIBNLog2 <-
#   zeroinfl(data = data$train.freq, 
#            formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density | VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density, 
#            offset=log(Exposure),
#            dist="negbin") %T>%
#            {print(summary(.))}
# 
# # On vire density
# mod.freq$ZIBNLog3 <-
#   zeroinfl(data = data$train.freq, 
#            formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas  | VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas,
#            offset=log(Exposure), dist="negbin") %T>%
#            {print(summary(.))}

# Bon au final les zeroinfl c'est pas la joie, le poisson va très bien.

################################################
######################## Regrouppement des modèles fittés
################################################
# ajout des modèles fittés a la base de donnée : 
  
      # on commence par construire des stats sur les modèles 
      # Cette partie est un peu longue.
      dfMod <- 
      tibble(Nom = names(mod.freq)) %>%
      mutate(
        Mod = mod.freq,
        family = map(Mod,"family"),
        glance = Mod %>% map(glance),
        tidy = Mod %>% map(tidy),
        augment = Mod %>% map(augment),
        Modele="freq",
        Type="train"
      ) %T>% {print(.)}
    
      # on jointe les databases
      Databases %<>% full_join(dfMod)
      Databases[!grepl("Original",Databases$Nom),] %<>%
        mutate(data = Mod %>% map("data"))
      Databases %<>%
        {print(.)}

################################################
######################## Selection d'un modèle 
################################################

# AU vue de tout les modèles fittés, il faudrais définir un critère pour en choisir un. 
# je pense qu'un bon critère serais de regarder leur AIC et BIC respectifs, voir d'autres statistiques (null.deviance) ou autre.
# Si c'est le cas, on peut réflechir comme ça : 
Databases[!map_lgl(Databases$glance,is.null),] %>%
  select(-c(Modele,Type,data)) %>%
  unnest(glance) %>%
  arrange(AIC) # changer ici le critère de classement pour afficher les modèles dans un autre ordre.

# une fois le modèle choisis, mettre TRUE dans sa colonne selected : 
Databases[Databases$Nom=="poissonLog",]$Selected = TRUE



```

# Partie Couts moyens 


```{r "Création d'un modèle de cout moyen", output = FALSE}


################################################
######################## Setup de l'environnement
################################################
# 
# # Récupération de la bonne base de donnée  de fréquence 
# data$train.sev <- 
#   Databases %>% 
#   filter(Modele=="sev",Type=="train") %>%
#   .$data %>%
#   `[[`(1) # la première entré de la base c'est les data brutes dans modèle.
# 
# # On utilise ici une liste "mod.sev" qui contiendra tout les modèles qu'on va fabriquer sur la partie frequence.
# mod.sev <- list()





#On entraine un premier modèle random, histoire de pouvoir prédire par la suite
mod.sev$gammalog1 <- data$train.sev %>% 
  glm (data = ., ClaimAmount ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas, family=Gamma(link=log)) %T>%
  {print(summary(.))}

cat("Modèle très qualitatif\n")
cat("Modèle très nerveux\n")

################################################
######################## Regrouppement des modèles fittés
################################################
# ajout des modèles fittés a la base de donnée : 
      dfMod <- 
      tibble(Nom = names(mod.sev)) %>%
      mutate(
        Mod2 = mod.sev,
        Modele="sev",
        Type="train"
      ) %T>% {print(.)}
    
      # on jointe les databases
      Databases %<>% full_join(dfMod)
      Databases[which(Databases$Modele=="sev" & Databases$Nom!="Original.dataset"),] %<>%
        mutate(Mod = map(Mod2,function(x) x)) %>%
        mutate(data = Mod %>% map("data")) %>%
        mutate(family = map(Mod,"family")) %>%
        mutate(glance = Mod %>% map(function(x){if(is.null(x)){return(NULL)}else{return(glance(x))}})) %>%
        mutate(tidy = Mod %>% map(function(x){if(is.null(x)){return(NULL)}else{return(tidy(x))}})) %>%
        mutate(augment = Mod %>% map(function(x){if(is.null(x)){return(NULL)}else{return(augment(x))}}))
       Databases %<>% select(-Mod2) %T>% {print(.)}

################################################
######################## Selection d'un modèle 
################################################
# AU vue de tout les modèles fittés, il faudrais définir un critère pour en choisir un. 
# je pense qu'un bon critère serais de regarder leur AIC et BIC respectifs, voir d'autres statistiques (null.deviance) ou autre.
# Si c'est le cas, on peut réflechir comme ça : 
Databases[which(Databases$Modele=="sev" & Databases$Nom!="Original.dataset"),] %>%
  select(-c(Modele,Type,data)) %>%
  unnest(glance) %>%
  arrange(AIC) # changer ici le critère de classement pour afficher les modèles dans un autre ordre.

# une fois le modèle choisis, mettre TRUE dans sa colonne selected : 
Databases[Databases$Nom=="gammalog1",]$Selected = TRUE




```


```{r "Prédictions"}

#Seulement sur base des models selectionnés

# commençons par récupérer les modèles : 
selec.freq <- Databases %>% filter(Modele=="freq",Type=="train",Selected==TRUE) %>% .$Mod
selec.sev <- Databases %>% filter(Modele=="sev",Type=="train",Selected==TRUE) %>% .$Mod

# puis les data : 
data$test.freq <- Databases %>% filter(Nom=="Original.dataset",Type=="test",Modele=="freq") %>% .$data
data$test.sev <- Databases %>% filter(Nom=="Original.dataset",Type=="test",Modele=="sev") %>% .$data

prediction.data$test.freq <- predict.glm(object = selec.freq, newdata = data$test.freq, type = "response")
length(prediction.data$test.freq)


prediction.data$test.sev <- predict.glm(object = selec.sev, newdata = data$test.sev, type = "response")
length(prediction.data$test.sev)

#Construisons la table prédiction VS réalité pour la fréquence :

#Si t'es dans data$test.sev t'es dans data$test.freq? 
length(data$test.sev$IDpol %in% data$test.freq$IDpol) == length(data$test.sev$IDpol)

# a fignoler, voir meme afaire :P

```



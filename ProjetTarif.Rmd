---
title: "Projet de tarification"
author: "Mikael BOZON, Pierre MARJOLET, William LAURENT, Oskar LAVERNY"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE,}
### Options knitR par d?ffaut pour tout les chunks : 
    knitr::opts_chunk$set(echo = FALSE,warning = FALSE)

### Instalation des packages n?c?ssaires : si vous avez besoin de CASdataset d?commentez la ligne : 
     #install.packages("CASdatasets", repos = "http://dutangc.free.fr/pub/RRepos/", type="source") # ligne sp?ciale pour CASdatasets qui ne vient pas de la meme source (pas de CRAN)
    .list.of.packages <- c("CASdatasets","dplyr", "xts", "sp", "magrittr","ggplot2","gridExtra","gtable","grid","tidyr", "purrr","broom","pscl","forcats")
    .new.packages <- .list.of.packages[!(.list.of.packages %in% installed.packages()[,"Package"])]
    if(length(.new.packages)) install.packages(.new.packages)
    lapply(.list.of.packages,function(x){library(x,character.only=TRUE)}) 
### Formating color in markdown : doit être utiliser comme ?a : `r colFmt("MY RED TEXT",'red')`
    .colFmt = function(x,color){
      outputFormat = rmarkdown::all_output_formats(knitr::current_input())#opts_knit$get("rmarkdown.pandoc.to")
      if(outputFormat == 'pdf_document')
        paste("\\textcolor{",color,"}{",x,"}",sep="")
      else if(outputFormat == 'html_document')
        paste("<font color='",color,"'>",x,"</font>",sep="")
      else
        x
      print("warnings dont matter, the point was to use the first condition")
    }

library(corrplot)
    
```

# TARIFICATION PAR MODELES LINEAIRES GENERALISES EN ASSURANCE NON VIE : Consignes

**Objectif** : proposer un tarif en appliquant diff?rentes m?thodes de tarification (fréquence-coût moyen ou non) vues en cours impliquant l'usage des modèles linéaires généralisés (GLM). Ce tarif pourra (ou non) être ségment? par profil pour tenir compte de l'apétence du risque.

**Logiciel utilis?** : logiciel R (logiciel libre Open Source de statistiques).

**Contexte** : ? partir d'une base sinistre d'&#39;'un portefeuille r?el d&#39;assurance non vie, plus précisément en assurance automobile (garantie Responsabilité Civile). Les bases SINISTRE et CONTRAT ? étudier sont contenues dans la librairie **CASdatasets** de R.

Les données ? elles s'intitulent **freMTPL2freq** et **freMTPL2sev**.

La signification des variables est fournie dans les fichiers d'aide de R.

Les caractéristiques sur les véhicules et les assurés ont été collect?es lors de la phase de souscription des contrats.

`r .colFmt("La façon de mener le projet (méthodes envisagées, testées, appréciation des résultats) est volontairement laissée très libre afin de responsabiliser les étudiants et leur permettre de développer un esprit critique.",'red')`



**Informations pratiques** : le projet se fait par groupe de 4 (aucun travail impliquant un nombre différent d'&#39;'étudiants ne sera noté, hormis pour un groupe si le nombre d'étudiants global n'est pas un multiple de 4). Il est à rendre pour le 14 janvier 2018 avril minuit maximum en renvoyant à l'adresse [xavier.milhaud@univ-lyon1.fr](mailto:xavier.milhaud@univ-lyon1.fr) les documents suivants:

- --un rapport qui décrit votre approche et vos résultats,
- --le script R **commenté** qui vous a servi à obtenir vos résultats.

Ce rapport ne devra pas dépasser 6 pages (sans compter la page de garde si vous en mettez une, et hors annexes éventuelles où des graphiques ou autres informations peuvent être insérés). La taille des annexes est également limitée à 5 pages.

**Les fichiers envoyés devront être nommés de la façon suivante: nom1-nom2-nom3-nom4.extension**

**`r .colFmt("Vous devez vous inspirer du travail réalisé en TP de tarification.",'red')`**

**Etapes du projet** : fusionner les bases &quot;Sinistres&quot; et &quot;Contrats&quot; de manière à faire le lien entre les montants de sinistre et les contrats qui y sont liés.



Voici les grandes étapes attendues à titre indicatif, mais vous pouvez bien sûr développer d'autres étapes personnelles:

1. 1)Proposer des statistiques descriptives sur l'ensemble des informations de la base de données: tableaux d'effectifs; indicateurs statistiques classiques; indicateurs de corrélation; densités… En déduire des messages et des actions à mener pour la modélisation à venir.

1. 2)Estimer des modèles de tarification pour expliquer la charge sinistre en fonction des facteurs de risque. Sur la base d'un raisonnement justifié et clair, optimiser le modèle de façon à retenir le modèle qui vous parait le meilleur selon un critère que vous expliciterez.

1. 3)Vous ne retiendrez à la fin qu'une unique modélisation parmi l'ensemble des types de modélisation testées. Expliquez et justifiez votre choix.

1. 4)En déduire un tarif individualisé en fonction des caractéristiques des assurés. Interpréter les résultats de manière concrète, et comparer ces résultats à votre historique.

1. 5)Apportez une vision critique de votre modélisation, et donnez des pistes d'amélioration potentielles si cela est possible.



# TARIFICATION PAR MODELES LINEAIRES GENERALISES EN ASSURANCE NON VIE : Réalisation


## Récupération des données

Les bases SINISTRE et CONTRAT à étudier sont contenues dans la librairie **CASdatasets** de R. Les données à importer s'intitulent **freMTPL2freq** et **freMTPL2sev**.

```{r Récupération des données,output=FALSE}

# On récupère les deux datasets : 
  data(freMTPL2freq)
  data(freMTPL2sev)

```


On a donc à notre disposition les variables suivantes : 


* Dans la base sinistre : 
    * IDpol : Identifiant police > ne serivra pas a l'analayse, juste a merger les deux bases.
    * ClaimNb : Nombre de sinistre déclarés sur la periode d'exposition > Reste un nombre, variable a prédire pour la partie fréquance.
    * Exposure : Durée d'exposition, en années > sera un Offset pour la partie fréquance.
    * Area : Inditifiant "Area code" > déja un factor, peut servir a faire un zonnier, mais pas obligatoire.
    * VehPower : Puissance du véhicule, ordonnée en catégories > doit être un factor, variable explicative.
    * VehAge : L'age du véhicule, en années > on peut faire des classes d'age pour récupérer un factor
    * DrivAge : L'age du conducteur (en france, on peut conduire a partir de 18ans) > Idem, des classes d'ages
    * BonusMalus : Coefficient Bonus/malus, entre 50 et 350. > On peut le garder en integer, je ne sais s'il faut el considérer comme une vairbale explicative, ou plutot l'appliquer a la fin sur la prime.
    * VehBrand : La marque du véhicule (catégories inconues) > reste un factor
    * VehGas : le type de carburant, "Diesel" ou "Regular" > devient un factor
    * Density : Nombre d'habitant par km2 dans la ville ou habite le conducteur > aucune idée, peut serivr a plusieurs choses.
    * Region : Les régions de polices, basées sur la classification standard française > clairment un facteur, sert a faire un zonnier.
* Dans la base contrat : 
    * IDpol : Identifiant police
    * ClaimAmout : Cout total du sinistre, vu a une date récente ( à l'ultime).
    
    
## Création d'une base de test et d'une base de train


```{r "Jonction des 2 bases"}

################################################
######################## Récupération des données et Jonction des deux bases et typage
################################################
  data.freq <- freMTPL2freq
  data.sev <- freMTPL2sev
#  data.full <- full_join(data.freq,data.sev)

#Suivant la modélisation retenue (glm brutal ou décomposition fréquence-coût moyen), nous aurons besoins d'utiliser les sinistres cumulés ou la moyenne des sinistres pour chaque police
sin.cum <- aggregate(ClaimAmount ~ IDpol, data = data.sev, sum)
names(sin.cum) <- c("IDpol", "Cum_ClaimAmount")
sin.mean <- aggregate(ClaimAmount ~ IDpol, data = data.sev, mean)
names(sin.mean) <- c("IDpol", "Mean_ClaimAmount")

#Il ne sous reste plus qu'à joindre la base de frequence avec les 2 autres bases de données

data.full <- merge(merge(data.freq, sin.cum, by="IDpol", all.x=TRUE), sin.mean, by="IDpol", all.x=TRUE)

#Les polices non sinistrées présentent logiquement la valeur NA comme montant moyen et cumulé de sinistres.
#Il nous faut les remplacer par 0.
data.full$Cum_ClaimAmount <- replace(data.full$Cum_ClaimAmount, is.na(data.full$Cum_ClaimAmount), 0)
data.full$Mean_ClaimAmount <- replace(data.full$Mean_ClaimAmount, is.na(data.full$Mean_ClaimAmount), 0)


#Pour autant, des polices sinistrées demeurent avec des montants moyen et cumulé de sinistres nuls
#Nous estimons que ce sont des erreurs dans la base de données et supprimons donc les lignes concernées
data.full <- data.full[-which(data.full$ClaimNb !=0 & data.full$Cum_ClaimAmount==0),]




################################################
######################## Typage des données
################################################

sapply(data.full,class)
data.full$ClaimNb <- as.numeric(data.full$ClaimNb) #le nombre de sinistres doit naturellement être de type numérique
data.full$IDpol <- as.factor(data.full$IDpol) #l'identifiant de la police doit être une variable catégorielle
data.full$VehPower <- as.numeric(data.full$VehPower) #de même pour la puissance du véhicule
data.full$VehAge <- as.numeric(data.full$VehAge) #l'âge est toujour numérique
data.full$DrivAge <- as.numeric(data.full$DrivAge) #idem
data.full$BonusMalus <- as.numeric(data.full$BonusMalus) #le Bonus Malus est un coefficient qui s'applique à la prime, donc est numérique
data.full$VehGas <- as.factor(data.full$VehGas) #le carburant de du véhicule est une variable catégorielle
data.full$Density <- as.numeric(data.full$Density) #la densité est de type numérique

sapply(data.full,class)

```






```{r "Analyse des données"}

################################################
######################## Correction des données aberrantes
################################################

#Les données sont annuelles, il est donc anormal de rencontrer des expositions > 1
#Nous supprimons donc toutes les polices présentant une exposition > 1

data.full <- data.full[-which(data.full$Exposure > 1),]

################################################
######################## Etude de la corrélation des données
################################################

###____ Entre variables quantitatives

str(data.full)
cor_matrix <- cor(data.full[,c(2,3,5,6,7,8,11,13,14)], method = "pearson")

diag(cor_matrix) <- 0

corrplot(cor_matrix)

#Il est intéressant de remarquer que le BonusMalus est positivement corrélé à l'âge du conducteur. Il ne faut pas perdre à l'esprit que le bonus malus se calcule sur la sinistralité passée. Il donne donc une information sur la sinistralité antérieur du conducteur, qui est donc plus susceptible d'être importante s'il est âgé.
#Les variables Cum_ClaimAmount et Mean_ClaimAmount sont logiquement très corrélées

###____ Entre variables qualitatives -> test du khi2


donnees_quali <- data.full[, c(4,9,10,12)]
str(donnees_quali)

nb <- ncol(donnees_quali)-1
test_ind <- matrix(0, nrow = nb, ncol = nb)
for (i in 1:nb){
  for (j in (i+1):(nb+1)){
    test_ind[i,j-1] <- chisq.test(table(donnees_quali[,i],donnees_quali[,j]))$p.value
  }
}
colnames(test_ind) <- colnames(donnees_quali[,-1])
rownames(test_ind) <- colnames(donnees_quali[,-4])
abs(test_ind)>0.05
#elles sont toutes indépendantes


###___ Analyse en composante principale

#A faire


```


```{r "Data management :creation du jeu d'apprentissage et du jeu de test & petites clarifications"}

################################################
######################## Crétation d'une base de test et d'une base de train
################################################

# paramètre : 
  set.seed(seed=100)
  .Proportion.Wanted = 0.10 # pour des question de rapiditée d'exection, j'ai déscendu la proportion a 0.01, il faut la remonter a 0.8 avent de rendre le code.

# application : 

  #data.freq$ClaimNb n'est pas un vecteur de la même taille que le jeu de données. réctifions
  data.full$ClaimNb <- as.vector(data.full$ClaimNb)

  #Je fais une liste d'éléments pris au hazard dans les indices de notre BDD de fréquence
  .index_entrainement <- (1:nrow(data.full)) %>% sample(.,size = .Proportion.Wanted * nrow(data.full))
  
  train.full <- data.full[.index_entrainement,]
  test.full <- data.full[! seq(from = 1, to = nrow(data.full)) %in% .index_entrainement, ]
  
# retour : 
  .Proportion.Achieved = round(100* nrow(train.full) / nrow(data.freq), 2)
  

# Les dataset data.freq et data.sev sont donc les originaux, non modifiés.
  
################################################
######################## Séparation des bases cout-moyen et des bases fréquences, nétoyage des bases.
################################################

#  .Clean <- . %>% drop_na %>% distinct

#  train.freq <- train.full %>%
#    select(-c(ClaimAmount)) %>%
#    .Clean
  
#  test.freq <- test.full %>% 
#    select(-c(ClaimAmount)) %>%
  #   .Clean
  # 
  # train.sev <- train.full %>%
  #   select(-c(ClaimNb)) %>%
  #   .Clean
  # 
  # test.sev <- test.full %>% 
  #   select(-c(ClaimNb)) %>%
  #   .Clean 

################################################
######################## Rassemblement des bases dans une seule variable Databases
################################################
  
  
  # L'idée : avoir tout au même endroit. 
  # On mettrat aussi les modèles dans ce dataset, ça permetra de pas se faire chier pour construire les predictions a la fin.
  # 
  # 
  # .MyList = list(data.freq,data.sev,data.full,train.full,train.sev,train.full,test.freq,test.sev,test.full)
  # names(.MyList)=c("data.freq","data.sev","data.full","train.full","train.sev","train.full","test.freq","test.sev","test.full")
  # Databases <-
  #   tibble(Nom = names(.MyList)) %>%
  #   mutate(
  #     data = .MyList,
  #     Type=map_chr(Nom,function(x) unlist(strsplit(x,"[.]"))[1]),
  #     Modele=map_chr(Nom,function(x) unlist(strsplit(x,"[.]"))[2]),
  #     Nom = "Original.dataset",
  #     Selected=FALSE
  #   ) %>%
  #   select(Modele,Type,Nom,data,Selected) %>%
  #   arrange(Modele,Type) %T>%
  #   {print(.)}


```



Préliminairement aux fitting de modèles, il est important de séparer la base en deux : base d'entrainement et base de test.

Tout d'abord nous jointons les deux bases par un full_join, puis nous séparons en base de train et base de test. Ensuite, nous créons les bases de fréquence et cout moeyn pour fitter nos eux modèles indépendants : d'un coté les fréquence, pour laquelle nous droppons la colonne cout puis nous droppons toutes les lignes avec des na et les lignes en double, et de l'autre coté les couts pour laquelle nous droppons tout les couts avec des NA et la colonnne Nombre de sinistres, ainsi queles lignes en double.



La base etant assez volumineuse, nous choississons de conserver une proportion de `r .Proportion.Achieved` pour entrainer nos modèles. Ainsi, l'autre patie de la base servira a tester nos modèles avent, une fois valider le framework, de fitter nos modèles sur l'intégralité de la base. 




# Partie fréquence 

## Typage des champs et regrouppements préliminaires




Aprés analyse du dataset, nous avons discrétiser certains variables quatitatives via les classes de disccrétisation suivantes : 

* Pour l'age du véhicule : `r names(table(train.full$VehAge))`
* Pour l'age du conducteur  : `r names(table(train.full$DrivAge))`
* Pour la puissance du véhicule : `r names(table(train.full$VehPower))`
* Pour le carburant : `r names(table(train.full$VehGas))`

En effet, au vue des graphiques suivant donnant la densit?e de chaque Nombre de sinistre en fonction de ces variables, ces classifications nous ont parues logiques : 

```{r fig.align="center", fig.width=10}
# Affichage des différents regroupements de modalité effectués.
# lors de l'execution finale ou tout simplement pour voir ce qu'il c'est passer, décommenter les lignes suivantes : 

# grid.arrange(.VehAgeBefore, .VehAgeAfter, nrow=2, ncol=1) # clairement, toujours pas.
# grid.arrange(.DrivAgeBefore, .DrivAgeAfter, nrow=2, ncol=1) # clairement, la non plus.
# grid.arrange(.VehPowerBefore, .VehPowerAfter, nrow=2, ncol=1) # clairement, ça va pas non plus.
# grid.arrange(.VehBrandBefore, .VehBrandAfter, nrow=2, ncol=1) # clairement, ça va pas.
# grid.arrange(.AreaBefore, .AreaAfter, nrow=2, ncol=1) # clairement, ça va pas.
```


Maintenant que nos discrétisations sont faites, appliquons un relevel sur data.freq pour créer un profil de référence : On prend pour chaque variable la modalitée la plus représentée :


Une fois ces discrétisations primaire effectuées, nous allons essayer de fitter un modèle GLM log-poisson sur la fréquence. 

Des ramifications ent erme de Zero-inflated, de Over-dispersed quasi-poisson, de Negative binomial ou de tout cela en meme temps seront ensuite possible. 
Un ajout de la version de renormalisation utilisée en TD sera aussi possible. 

Si souhaiter, on pourra aussi mettre a par la variable géographique pour la traiter en terme de zonnier. 

On va donc fitter le nombre de sinistres sur le reste des variables. 


Il faudrais aussi mettre en place un échantillon de validation et un échantillon d'apprentissagE. 
Les choix de ces échantillons peuvent être faits par bootstrap, par exemple : 
On choisis aléatoirement des échantillons, on fitte les modèles sur ces échantillons et on prend en modèle moyen. 

Une sorte de GLM bootstrapé. Why not :)

```{r "Fitting du modèle de fréquence",include=FALSE}

#on definit une base d'apprentissage et une de test pour la fréquence
data.freq.train <- train.full
data.freq.test <- test.full





#######_____________     Cette partie du code est à reprendre, il faut faire suivre les regroupements de modalité sur la base de test



################################################
######################## Preliminaires aux regrouppements de modalités
################################################

 
# De plus, j'ai séparer dans deux chunks les regrouppements de modalitées et les fitting de modèles. Pas sur qu'il fallais faire comme ça. J'ai peut d'avoir trop regroupper car mes modèles fittent très bien. Ou alors c'est juste que la BDD est enorme. 

# Commençons par réduire la taille de la data pour pouvoir travailler tranquilement si besoin. (pas obligatoire)

# Re-factorisons certaines variables :
#  str(data.freq.train)
#  cols <- c("VehGas")
#  data.freq.train[cols] <- lapply(data.freq.train[cols], function(x) factor(x))
#  str(data.freq.train)

#par(mfrow=c(2,2))

# Fonction qui fabrique les plots expliquant les regrouppements de modalités choisis : prérequis : 
makeplot <- function(data,var,title="Before",continuous=TRUE){
  
  p <- data %>% ggplot(aes(x = data[,var],weight = Exposure,fill=as.factor(ClaimNb)), environment = environment())
  
  if(continuous){p = p + geom_histogram()}
  else          {p = p + geom_bar()}
  
  p = p + ggtitle(title) + labs(fill="Nombre de Sinitres",x=var,y="Exposition Totale")  
  return(p)
}


################################################
######################## Regrouppement de l'age du v?hicule
################################################
# Graphique de l'?tat du monde : 
data.freq.train %>% makeplot("VehAge") -> .VehAgeBefore
  
# Ok donc on peut clairement cr?er une classe 30+
#df[df$VehAge<30,] %>% 
#  qplot(data=.,VehAge,fill = as.factor(ClaimNb)) 
#et des classes plus petites de 5 ann?es chacunes entre 0 et 20, puis 20-30.

data.freq.train$VehAge %<>% 
  cut(., breaks = c(0, 5, 10, 15, 20, 30, max(.)), include.lowest = TRUE) %T>%
  {print(summary(.))}

# Graphique de l'?tat du monde Apr?s : 
data.freq.train %>% makeplot("VehAge",title = "After",continuous=FALSE) -> .VehAgeAfter 

################################################
######################## Regrouppement de l'Age du conducteur
################################################
# Graphique de l'?tat du monde : 
data.freq.train %>% makeplot("DrivAge") -> .DrivAgeBefore
# Ok ce coup ci c'est moins flagrant. 
# On va faire des classes d'age classqiue 18-25 puis 25-35,35-45, etc. jusqu'a 75+ ou 85+ en fonction du nombre de pelo

#df[df$DrivAge > 75,] %>% 
#  qplot(data=.,DrivAge,fill = as.factor(ClaimNb)) # Ok donc on prend 85+, mais peut-être qu'on les rassemblera plus-tard (pour des problème de manque de donn?es.)

data.freq.train$DrivAge %<>% 
  cut(., breaks = c(18, seq(from = 25,to = 85,by = 10), max(.)), include.lowest = TRUE) %T>%
  {print(summary(.))}

# Graphique de l'?tat du monde Apr?s : 
data.freq.train %>% makeplot("DrivAge",title="After",continuous=FALSE) -> .DrivAgeAfter 
################################################
######################## Regrouppement de la puissance du v?hicule
################################################
data.freq.train %>% makeplot("VehPower") -> .VehPowerBefore 

data.freq.train$VehPower %<>% 
  cut(., breaks = c(4,5,6,7,8,11,15), include.lowest = TRUE,right=FALSE) %T>%
  {print(summary(.))}

data.freq.train %>% makeplot("VehPower",title="After",continuous=FALSE) -> .VehPowerAfter

################################################
######################## Regrouppement d'Area
################################################
data.freq.train %>% makeplot("Area",continuous=FALSE) -> .AreaBefore 

data.freq.train$Area %<>% 
  fct_collapse("AB" = c("A","B"),
               "EF" = c("E","F")) %T>%
  {print(summary(.))}

data.freq.train %>% makeplot("Area",title="After",continuous=FALSE) -> .AreaAfter



################################################
######################## Regrouppement de la marque du véhicule 
################################################
# Commençons par les normaliser : 

data.freq.train$VehBrand %<>% 
  as.character %>% 
  gsub("B", "", .) %>%
  {as.numeric(.)}

table(data.freq.train$VehBrand)

# Puis réduisons le nombre de modalitées : Il semble en effet que les codes soit ordonnés.
data.freq.train %>% makeplot("VehBrand",continuous=FALSE) -> .VehBrandBefore

# APrès un premier regrouppement 2 par 2 qui ne fittais pas sur la fréquence, on tente unregrouppement en 3 classes : 
table(data.freq.train$VehBrand)
data.freq.train$VehBrand %<>% 
  cut(., breaks = c(1,3,7,11), include.lowest = TRUE,right=FALSE) %T>%
  {print(summary(.))}


data.freq.train %>% makeplot("VehBrand",title="After",continuous=FALSE) -> .VehBrandAfter
# Les variables créers VariableBefore et VariableAfter contienne des graphiques, que l'on plottera plus tard. 
# Les modifications des variables ont été dirrectement appliquées à la base de données 'df', ces variables servent juste a l'affichage.
  
################################################
######################## Création du profil de référence
################################################
# Le but va être de relevel automatique tout les factor du dataset sur la valeur la plus représentée : 

# petite fonction : 
Autorelevel <- function(dataset){
  
    # On commence par construire le profil de référence.
    .ProfilDeRef <- 
      dataset %>%
      Filter(is.factor,.) %>%
      map(table) %>%
      map(sort,decreasing = TRUE) %>%
      map(names) %>%
      map_chr(1)
    
    # Puis pour chaque facteur, on relever sur le profil de référence.
    for (i in names(Filter(is.factor,dataset))){
      dataset[,i] <- relevel(dataset[,i],.ProfilDeRef[i])
    }
    
    return(dataset)
}

data.freq.train %<>% Autorelevel


#Reprendre le setup de l'environnement

################################################
######################## Setup de l'environnement
################################################

# # Récupération de la bonne base de donnée  de fréquence 
# train.freq <- 
#   Databases %>% 
#   filter(Modele=="freq",Type=="train") %>%
#   .$data %>%
#   `[[`(1)

# On utilise ici une liste "mod.freq" qui contiendra tout les modèles qu'on va fabriquer sur la partie frequence.
mod.freq <- list()

################################################
######################## Modèles Poissoniens
################################################

# Attention, on met l'exposure en offset 

#names(data.freq.train)
# IDpol + ClaimNb + Exposure + Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region

# Passons a la BDD avec les regrouppements de modalité appliqué : Par déffaut on vire IDPOL

mod.freq$poissonLog<- data.freq.train %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# on vire dirrectement la région : 
mod.freq$poissonLog2<- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# ou alors plutot la puissance du véhicule : 
mod.freq$poissonLog2b<- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region  , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# ha c'est mieux. 
# Et si on vire les deux : 
mod.freq$poissonLog2c<- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density, offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}
# c'est encore mieux.
  
# retirons density : 
mod.freq$poissonLog3 <- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehBrand + VehGas, offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}

# retirons la marque du véhicule : 
mod.freq$poissonLog4 <- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehAge + DrivAge + BonusMalus + VehGas, offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}
#Ok c'est un tout petit peu mieux. Density explique probablement AreaF, ou en tout cas elles apporte la meme information.

# mais tout le reste est plutot bon. 

################################################
######################## Modèles poissoniens surdispersés
################################################
mod.freq$odPoissonLog<- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas , offset=log(Exposure), family=quasipoisson(link=log)) %T>%
  {print(summary(.))}

# le paramètre de dispertion est très proche de 1 !!! 
# C'est OUF, c'est très rare d'avoir des données qui ne sont pas overdispersé.

# on a es problèmes avecles Area, car les AreaB et AreaF sont moins représentées que les autres. 
# il faudrais peut-être regroupper.
#Bon OK meme en regrouppant ça fitte pas. Donc on les dégage.
mod.freq$odPoissonLog2<- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas , offset=log(Exposure), family=quasipoisson(link=log)) %T>%
  {print(summary(.))}

mod.freq$PoissonLog4<- train.freq %>% 
  glm (data = ., ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas , offset=log(Exposure), family=poisson(link=log)) %T>%
  {print(summary(.))}


################################################
######################## Modèles ZIP
################################################

# mod.freq$ZIPLog <-
#   zeroinfl(data = train.freq, formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region, offset=log(Exposure), dist="poisson") %T>%
#   {print(summary(.))}

################################################
######################## Modèles ZIBN
################################################
# # Modèle complet : ( attention trèèèèès long a fitter)
# mod.freq$ZIBNLog <-
#   zeroinfl(data = train.freq, 
#            formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density + Region, offset=log(Exposure), dist="negbin") %T>%
#            {print(summary(.))}
# 
# 
# # On vire Area dans le modèle de zero-inflation et Region des deux cotés.
# mod.freq$ZIBNLog2 <-
#   zeroinfl(data = train.freq, 
#            formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density | VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Density, 
#            offset=log(Exposure),
#            dist="negbin") %T>%
#            {print(summary(.))}
# 
# # On vire density
# mod.freq$ZIBNLog3 <-
#   zeroinfl(data = train.freq, 
#            formula = ClaimNb ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas  | VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas,
#            offset=log(Exposure), dist="negbin") %T>%
#            {print(summary(.))}

# Bon au final les zeroinfl c'est pas la joie, le poisson va très bien.

################################################
######################## Regrouppement des modèles fittés
################################################
# ajout des modèles fittés a la base de donnée : 
  
      # on commence par construire des stats sur les modèles 
      # Cette partie est un peu longue.
      dfMod <- 
      tibble(Nom = names(mod.freq)) %>%
      mutate(
        Mod = mod.freq,
        family = map(Mod,"family"),
        glance = Mod %>% map(glance),
        tidy = Mod %>% map(tidy),
        augment = Mod %>% map(augment),
        Modele="freq",
        Type="train"
      ) %T>% {print(.)}
    
      # on jointe les databases
      Databases %<>% full_join(dfMod)
      Databases[!grepl("Original",Databases$Nom),] %<>%
        mutate(data = Mod %>% map("data"))
      Databases %<>%
        {print(.)}

################################################
######################## Selection d'un modèle 
################################################

# AU vue de tout les modèles fittés, il faudrais définir un critère pour en choisir un. 
# je pense qu'un bon critère serais de regarder leur AIC et BIC respectifs, voir d'autres statistiques (null.deviance) ou autre.
# Si c'est le cas, on peut réflechir comme ça : 
Databases[!map_lgl(Databases$glance,is.null),] %>%
  select(-c(Modele,Type,data)) %>%
  unnest(glance) %>%
  arrange(AIC) # changer ici le critère de classement pour afficher les modèles dans un autre ordre.

# une fois le modèle choisis, mettre TRUE dans sa colonne selected : 
Databases[Databases$Nom=="poissonLog",]$Selected = TRUE



```

# Partie Couts moyens 


```{r "Création d'un modèle de cout moyen", output = FALSE}


################################################
######################## Setup de l'environnement
################################################
# 
# # Récupération de la bonne base de donnée  de fréquence 
# train.sev <- 
#   Databases %>% 
#   filter(Modele=="sev",Type=="train") %>%
#   .$data %>%
#   `[[`(1) # la première entré de la base c'est les data brutes dans modèle.
# 
# # On utilise ici une liste "mod.sev" qui contiendra tout les modèles qu'on va fabriquer sur la partie frequence.
# mod.sev <- list()





#On entraine un premier modèle random, histoire de pouvoir prédire par la suite
mod.sev$gammalog1 <- train.sev %>% 
  glm (data = ., ClaimAmount ~ Area + VehPower + VehAge + DrivAge + BonusMalus + VehGas, family=Gamma(link=log)) %T>%
  {print(summary(.))}

cat("Modèle très qualitatif\n")
cat("Modèle très nerveux\n")

################################################
######################## Regrouppement des modèles fittés
################################################
# ajout des modèles fittés a la base de donnée : 
      dfMod <- 
      tibble(Nom = names(mod.sev)) %>%
      mutate(
        Mod2 = mod.sev,
        Modele="sev",
        Type="train"
      ) %T>% {print(.)}
    
      # on jointe les databases
      Databases %<>% full_join(dfMod)
      Databases[which(Databases$Modele=="sev" & Databases$Nom!="Original.dataset"),] %<>%
        mutate(Mod = map(Mod2,function(x) x)) %>%
        mutate(data = Mod %>% map("data")) %>%
        mutate(family = map(Mod,"family")) %>%
        mutate(glance = Mod %>% map(function(x){if(is.null(x)){return(NULL)}else{return(glance(x))}})) %>%
        mutate(tidy = Mod %>% map(function(x){if(is.null(x)){return(NULL)}else{return(tidy(x))}})) %>%
        mutate(augment = Mod %>% map(function(x){if(is.null(x)){return(NULL)}else{return(augment(x))}}))
       Databases %<>% select(-Mod2) %T>% {print(.)}

################################################
######################## Selection d'un modèle 
################################################
# AU vue de tout les modèles fittés, il faudrais définir un critère pour en choisir un. 
# je pense qu'un bon critère serais de regarder leur AIC et BIC respectifs, voir d'autres statistiques (null.deviance) ou autre.
# Si c'est le cas, on peut réflechir comme ça : 
Databases[which(Databases$Modele=="sev" & Databases$Nom!="Original.dataset"),] %>%
  select(-c(Modele,Type,data)) %>%
  unnest(glance) %>%
  arrange(AIC) # changer ici le critère de classement pour afficher les modèles dans un autre ordre.

# une fois le modèle choisis, mettre TRUE dans sa colonne selected : 
Databases[Databases$Nom=="gammalog1",]$Selected = TRUE




```


```{r "Prédictions"}

#Seulement sur base des models selectionnés

# commençons par récupérer les modèles : 
selec.freq <- Databases %>% filter(Modele=="freq",Type=="train",Selected==TRUE) %>% .$Mod
selec.sev <- Databases %>% filter(Modele=="sev",Type=="train",Selected==TRUE) %>% .$Mod

# puis les data : 
test.freq <- Databases %>% filter(Nom=="Original.dataset",Type=="test",Modele=="freq") %>% .$data
test.sev <- Databases %>% filter(Nom=="Original.dataset",Type=="test",Modele=="sev") %>% .$data

prediction.test.freq <- predict.glm(object = selec.freq, newdata = test.freq, type = "response")
length(prediction.test.freq)


prediction.test.sev <- predict.glm(object = selec.sev, newdata = test.sev, type = "response")
length(prediction.test.sev)

#Construisons la table prédiction VS réalité pour la fréquence :

#Si t'es dans test.sev t'es dans test.freq? 
length(test.sev$IDpol %in% test.freq$IDpol) == length(test.sev$IDpol)

# a fignoler, voir meme afaire :P

```


